.....................................................................................
		  Event Sourcing with Kafka( Any Message Broker)
.....................................................................................
Storing events into db is not suitable most of the use cases,so we need to store events into event store.

Event Store:
  The software/infrastructure which primarily designed for storing events.

Event Store Products:

1.Apache/Confluent Kafka
   Most popular events storage software.

2.event store: https://www.eventstore.com/
  It is also one of the software Primiraly used to store events.

3.Most of the cloud providers also offers event storage
  eg
   google pub-sub.

4.RabbitMq, IBM MQ

5.Redis message Broker
etc....

Our implementation is Kafka.
.....................................................................................		         Domain Event and Event Sourcing Design Pattern
			      Implementation
		 (SmallRye Reactive Messaging Specification)
.....................................................................................
In spring we have Spring Cloud Stream..

SmallRye Reactive Messaging Specification:
..........................................
SmallRye Reactive Messaging is a framework for building event-driven, data streaming, and event-sourcing applications using CDI.

 It lets your application interaction using various messaging technologies such as Apache Kafka, AMQP or MQTT. The framework provides a flexible programming model bridging CDI and event-driven.


Our Implementation Could be Apache Kafka:
..........................................

Apache Kafka is a popular open-source distributed event streaming platform. It is used commonly for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. Similar to a message queue, or an enterprise messaging platform, it lets you:


1.publish (write) and subscribe to (read) streams of events, called records.

2.store streams of records durably and reliably inside topics.

3.process streams of records as they occur or retrospectively.


Core Concepts of SmallRye Reactive Messaging:
....................................

1.Message:
..........
Applications send and receive messages. A message wraps a payload and can be extended with some metadata. With the Kafka connector, a message corresponds to a Kafka record.


2.Channels:
 Messages transit on channels. Application components connect to channels to publish and consume messages. The Kafka connector maps channels to Kafka topics.

3.Connectors:
  Channels are connected to message backends using connectors.
Connectors are configured to map incoming messages to a specific channel (consumed by the application) and collect outgoing messages sent to a specific channel. Each connector is dedicated to a specific messaging technology. For example, the connector dealing with Kafka is named smallrye-kafka.

....................................................................................
			 Connectors

Connectors:
  Reactive Messaging can handle messages generated from within the application but also interact with remote brokers. Reactive Messaging Connectors interacts with these remote brokers to retrieve messages and send messages using various protocols and technology.

Each connector handles to a specific technology. For example, a Kafka Connector is responsible for interacting with Kafka, while an MQTT Connector is responsible for MQTT interactions.

Connector name:
...............
Each connector has a name. This name is referenced by the application to indicate that this connector manages a specific channel.

For example, the SmallRye Kafka Connector is named: smallrye-kafka

Types of Connectors:
....................

Inbound and Outbound connectors

Connector can:
1.retrieve messages from a remote broker (inbound)

2.send messages to a remote broker (outbound)

3.A connector can, of course, implement both directions.InBound and OutBound.

.........................................................................

Role of InBound connector:
..........................

1.Inbound connectors are responsible for:

	1..1.Getting messages from the remote broker,

	1.2.Creating a Reactive Messaging "Message" associated with the retrieved 	message.

	1.3.Potentially associating technical metadata with the message.
	   It includes unmarshalling the payload.

	1.4.Associating an acknowledgment callback to acknowledge the incoming 	   message when the Reactive Messaging message is processed/acknowledged.


Role of OutBound Connector:

Outbound connectors are responsible for:

	1.1.Receiving Reactive Messaging Message and transform it into a structure             understood by the remote broker. It includes marshaling the payload.

	1.2..If the Message contains outbound metadata (metadata set during the 	     processing to influence the outbound structure and routing), taking them              into account.

        1.3.Sending the message to the remote broker.

	1.4. Acknowledging the Reactive Messaging Message when the broker has              accepted/acknowledged the message.

Configuring Connectors:
......................
Configuration is done in application.properties 

mp.messaging.[incoming|outgoing].[channel-name].[attribute]=[value]

eg:
mp.messaging.incoming.dummy-incoming-channel.connector=dummy (kafka)
mp.messaging.incoming.dummy-incoming-channel.attribute=value
 
mp.messaging.outgoing.dummy-outgoing-channel.connector=dummy (RabbitMq)
mp.messaging.outgoing.dummy-outgoing-channel.attribute=value


Mapping Channels In the Code:
.............................
  We use annotations to map channels

eg:
mp.messaging.incoming.prices.connector=smallrye-kafka       
mp.messaging.incoming.prices.value.deserializer=org.apache.kafka.common.serialization.DoubleDeserializer    
mp.messaging.incoming.prices.broadcast=true 


@Incoming:
//////////
The [incoming|outgoing] segment indicates the direction.

	an incoming channel consumes data from a message broker or something producing data.
     It’s an inbound interaction. It relates to methods annotated with an @Incoming using the same channel name.

@Outgoing:
	an outgoing consumes data from the application and forwards it to a message broker or something consuming data. 
	It’s an outbound interaction.
 It relates to methods annotated with an @Outgoing using the same channel name.

Channel Name:
..............
 The [channel-name] is the name of the channel. 
  If the channel name contains a . (dot), you would need to use " (double-quote) around it. For example, to configure the dummy.incoming.channel channel, you would need:

mp.messaging.incoming."dummy.incoming.channel".connector=dummy
mp.messaging.incoming."dummy.incoming.channel".attribute=value


Attributes:
..........
 The [attribute]=[value] sets a "specific connector" attribute to the given value. Attributes depend on the used connector. So, refer to the connector documentation to check the supported attributes.

https://smallrye.io/smallrye-reactive-messaging/latest/kafka/writing-kafka-records/#configuration-reference

Here is an example of a channel using an MQTT connector, consuming data from a MQTT broker, and a channel using a Kafka connector (writing data to Kafka):

# [Channel - health] - Consume data from MQTT

mp.messaging.incoming.health.topic=neo
mp.messaging.incoming.health.connector=smallrye-mqtt
mp.messaging.incoming.health.host=localhost
mp.messaging.incoming.health.broadcast=true
# [/Channel - health]

# [Channel - data] - Produce data to Kafka
mp.messaging.outgoing.data.connector=smallrye-kafka
mp.messaging.outgoing.data.bootstrap.servers=localhost:9092
mp.messaging.outgoing.data.key.serializer=org.apache.kafka.common.serialization.StringSerializer
mp.messaging.outgoing.data.value.serializer=io.vertx.kafka.client.serialization.JsonObjectSerializer
mp.messaging.outgoing.data.acks=1
# [/Channel - data]

...................................................................................
			  Receiving Messages from the Kafka
...................................................................................


1.Listener Pattern:
 Just declare method inside class , declare that method as listener... and read messages.


import org.eclipse.microprofile.reactive.messaging.Incoming;

import jakarta.enterprise.context.ApplicationScoped;

@ApplicationScoped
public class PriceConsumer {

    @Incoming("prices")
    public void consume(double price) {
        // process your price.
    }

}
Different ways of consuming messages:
.....................................

@Incoming("prices")
public CompletionStage<Void> consume(Message<Double> msg) {
    // access record metadata
    var metadata = msg.getMetadata(IncomingKafkaRecordMetadata.class).orElseThrow();
    // process the message payload.
    double price = msg.getPayload();
    // Acknowledge the incoming message (commit the offset)
    return msg.ack();
}

Kafka stores data as records:

if you want to access Kafka Records directly...

@Incoming("prices")
public void consume(ConsumerRecord<String, Double> record) {
    String key = record.key(); // Can be `null` if the incoming record has no key
    String value = record.value(); // Can be `null` if the incoming record has no value
    String topic = record.topic();
    int partition = record.partition();
    // ...
}

2.Dependency Injection Pattern:

 Channels can be injected into class and we can read messages...

import io.smallrye.mutiny.Multi;
import org.eclipse.microprofile.reactive.messaging.Channel;
import jakarta.inject.Inject;
import jakarta.ws.rs.GET;
import jakarta.ws.rs.Path;
import jakarta.ws.rs.Produces;
import jakarta.ws.rs.core.MediaType;
import org.jboss.resteasy.reactive.RestStreamElementType;

@Path("/prices")
public class PriceResource {

    @Inject
    @Channel("prices")
    Multi<Double> prices;

    @GET
    @RestStreamElementType(MediaType.TEXT_PLAIN)
    public Multi<Double> stream() {
        return prices;
    }
}

@Inject @Channel("prices") Multi<Double> streamOfPayloads;

@Inject @Channel("prices") Multi<Message<Double>> streamOfMessages;

@Inject @Channel("prices") Publisher<Double> publisherOfPayloads;

@Inject @Channel("prices") Publisher<Message<Double>> publisherOfMessages;
.....................................................................................

.....................................................................................
			 Sending Messages To Kafka
.....................................................................................

Configuration :

%prod.kafka.bootstrap.servers=kafka:9092 
mp.messaging.outgoing.prices-out.connector=smallrye-kafka 
mp.messaging.outgoing.prices-out.topic=prices 

prices-out - channel Name where we publish Records/Messages.

How to map out going channal.

import io.smallrye.mutiny.Multi;
import org.eclipse.microprofile.reactive.messaging.Outgoing;

import jakarta.enterprise.context.ApplicationScoped;
import java.time.Duration;
import java.util.Random;

@ApplicationScoped
public class KafkaPriceProducer {

    private final Random random = new Random();

    @Outgoing("prices-out")
    public Multi<Double> generate() {
        // Build an infinite stream of random prices
        // It emits a price every second
        return Multi.createFrom().ticks().every(Duration.ofSeconds(1))
            .map(x -> random.nextDouble());
    }

}

Note:
  You should not call methods annotated with @Incoming and @Outgoing directly from   your   code.

Note that the generate method returns a Multi<Double>, which implements the Reactive Streams Publisher interface. This publisher will be used by the framework to generate messages and send them to the configured Kafka topic.

Different Syntax:
@Outgoing("out")
public Multi<Record<String, Double>> generate() {
    return Multi.createFrom().ticks().every(Duration.ofSeconds(1))
        .map(x -> Record.of("my-key", random.nextDouble()));
}

@Outgoing("generated-price")
public Multi<Message<Double>> generate() {
    return Multi.createFrom().ticks().every(Duration.ofSeconds(1))
            .map(x -> Message.of(random.nextDouble())
                    .addMetadata(OutgoingKafkaRecordMetadata.<String>builder()
                            .withKey("my-key")
                            .withTopic("my-key-prices")
                            .withHeaders(new RecordHeaders().add("my-header", "value".getBytes()))
                            .build()));
}

...................................................................................
			How to push messages from the Rest api
....................................................................................

@Emitter
Sending messages with @Emitter:

Sometimes, you need to have an imperative way of sending messages.

For example, if you need to send a message to a stream when receiving a POST request inside a REST endpoint. In this case, you cannot use @Outgoing because your method has parameters.

For this, you can use an Emitter.

import org.eclipse.microprofile.reactive.messaging.Channel;
import org.eclipse.microprofile.reactive.messaging.Emitter;

import jakarta.inject.Inject;
import jakarta.ws.rs.POST;
import jakarta.ws.rs.Path;
import jakarta.ws.rs.Consumes;
import jakarta.ws.rs.core.MediaType;

@Path("/prices")
public class PriceResource {

    @Inject
    @Channel("price-create")
    Emitter<Double> priceEmitter;

    @POST
    @Consumes(MediaType.TEXT_PLAIN)
    public void addPrice(Double price) {
        CompletionStage<Void> ack = priceEmitter.send(price);
    }
}
....................................................................................

....................................................................................

How to send Message with ack?

import org.eclipse.microprofile.reactive.messaging.Channel;

import jakarta.inject.Inject;
import jakarta.ws.rs.POST;
import jakarta.ws.rs.Path;
import jakarta.ws.rs.Consumes;
import jakarta.ws.rs.core.MediaType;

import io.smallrye.reactive.messaging.MutinyEmitter;

@Path("/prices")
public class PriceResource {

    @Inject
    @Channel("price-create")
    MutinyEmitter<Double> priceEmitter;

    @POST
    @Consumes(MediaType.TEXT_PLAIN)
    public Uni<String> addPrice(Double price) {
        return quoteRequestEmitter.send(price)
                .map(x -> "ok")
                .onFailure().recoverWithItem("ko");
    }
}
.....................................................................................

I am going to create project using code.quarkus.io website, the reason is we will have boiler plate code for kafka.


Implementation:

    <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-messaging</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-messaging-kafka</artifactId>
       </dependency>


package org.acme;

import io.quarkus.runtime.StartupEvent;
import org.eclipse.microprofile.reactive.messaging.*;

import jakarta.enterprise.context.ApplicationScoped;
import jakarta.enterprise.event.Observes;
import jakarta.inject.Inject;
import java.util.stream.Stream;

@ApplicationScoped
public class MyMessagingApplication {

    @Inject
    @Channel("words-out")
    Emitter<String> emitter;

    /**
     * Sends message to the "words-out" channel, can be used from a JAX-RS resource or any bean of your application.
     * Messages are sent to the broker.
     **/
    void onStart(@Observes StartupEvent ev) {
        Stream.of("Hello", "with", "Quarkus", "Messaging", "message")
                .forEach(string -> emitter.send(string));
    }

    /**
     * Consume the message from the "words-in" channel, uppercase it and send it to the uppercase channel.
     * Messages come from the broker.
     **/
    @Incoming("words-in")
    @Outgoing("uppercase") //in memory channel- no physical topic is mapped in Kafka.
    public Message<String> toUpperCase(Message<String> message) {
        return message.withPayload(message.getPayload().toUpperCase());
    }

    /**
     * Consume the uppercase channel (in-memory) and print the messages.
     **/
    @Incoming("uppercase")
    public void sink(String word) {
        System.out.println(">> " + word);
    }
}

application.properties

kafka.bootstrap.servers=localhost:9092


quarkus.kafka.devservices.enabled=true
#Incoming

mp.messaging.incoming.words-in.connector=smallrye-kafka
mp.messaging.incoming.words-in.topic=words

#outgoing
mp.messaging.outgoing.words-out.connector=smallrye-kafka
mp.messaging.outgoing.words-out.topic=words
mp.messaging.incoming.words-in.auto.offset.reset=earliest


External kafka setup:
docker-compose.yml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - kafka-network

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    networks:
      - kafka-network

networks:
  kafka-network:

Kafka Client:
docker run -it --network container:kafka wurstmeister/kafka /bin/bash

root@80ee5f5e7083:/opt/kafka/bin# kafka-console-consumer.sh  --bootstrap-server kafka:9092 --topic words --from-beginning

.....................................................................................
		Event Sourcing with Reactive Messaging using Kafka
.....................................................................................

Note:
 Before we have seen how we can implement EventSourcing with Database 
 Now we are going to see how to push events into kafka.


docker-compose.yml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - kafka-network

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    networks:
      - kafka-network

  postgres:
    image: postgres:latest
    container_name: postgres_server
    environment:
      POSTGRES_USER: your_username
      POSTGRES_PASSWORD: your_password
      POSTGRES_DB: your_database
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin_client
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin_password
    ports:
      - "8080:80"

volumes:
  postgres_data:

networks:
  kafka-network:

pom.xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>org.acme</groupId>
    <artifactId>eventsourcing-kafka</artifactId>
    <version>1.0.0-SNAPSHOT</version>

    <properties>
        <compiler-plugin.version>3.13.0</compiler-plugin.version>
        <maven.compiler.release>17</maven.compiler.release>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
        <quarkus.platform.artifact-id>quarkus-bom</quarkus.platform.artifact-id>
        <quarkus.platform.group-id>io.quarkus.platform</quarkus.platform.group-id>
        <quarkus.platform.version>3.17.2</quarkus.platform.version>
        <skipITs>true</skipITs>
        <surefire-plugin.version>3.5.0</surefire-plugin.version>
    </properties>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>${quarkus.platform.group-id}</groupId>
                <artifactId>${quarkus.platform.artifact-id}</artifactId>
                <version>${quarkus.platform.version}</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <dependencies>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-rest</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-rest-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-messaging-kafka</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-reactive-pg-client</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-hibernate-reactive-panache</artifactId>
        </dependency>
        <dependency>
            <groupId>com.google.code.gson</groupId>
            <artifactId>gson</artifactId>
            <version>2.11.0</version>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>1.18.32</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-messaging</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-arc</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-junit5</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>io.rest-assured</groupId>
            <artifactId>rest-assured</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>${quarkus.platform.group-id}</groupId>
                <artifactId>quarkus-maven-plugin</artifactId>
                <version>${quarkus.platform.version}</version>
                <extensions>true</extensions>
                <executions>
                    <execution>
                        <goals>
                            <goal>build</goal>
                            <goal>generate-code</goal>
                            <goal>generate-code-tests</goal>
                            <goal>native-image-agent</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>${compiler-plugin.version}</version>
                <configuration>
                    <parameters>true</parameters>
                </configuration>
            </plugin>
            <plugin>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>${surefire-plugin.version}</version>
                <configuration>
                    <systemPropertyVariables>
                        <java.util.logging.manager>org.jboss.logmanager.LogManager</java.util.logging.manager>
                        <maven.home>${maven.home}</maven.home>
                    </systemPropertyVariables>
                </configuration>
            </plugin>
            <plugin>
                <artifactId>maven-failsafe-plugin</artifactId>
                <version>${surefire-plugin.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>integration-test</goal>
                            <goal>verify</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <systemPropertyVariables>
                        <native.image.path>${project.build.directory}/${project.build.finalName}-runner
                        </native.image.path>
                        <java.util.logging.manager>org.jboss.logmanager.LogManager</java.util.logging.manager>
                        <maven.home>${maven.home}</maven.home>
                    </systemPropertyVariables>
                </configuration>
            </plugin>
        </plugins>
    </build>

    <profiles>
        <profile>
            <id>native</id>
            <activation>
                <property>
                    <name>native</name>
                </property>
            </activation>
            <properties>
                <skipITs>false</skipITs>
                <quarkus.native.enabled>true</quarkus.native.enabled>
            </properties>
        </profile>
    </profiles>
</project>

application.properties
quarkus.http.port=8081
kafka.bootstrap.servers=localhost:9092
mp.messaging.incoming.words-in.auto.offset.reset=earliest
mp.messaging.incoming.words-in.topic=words
mp.messaging.outgoing.words-out.topic=words
mp.messaging.incoming.stock-in.connector=smallrye-kafka
mp.messaging.outgoing.stock.connector=smallrye-kafka
mp.messaging.outgoing.stock.topic=stock
mp.messaging.incoming.stock-in.topic=stock
mp.messaging.incoming.stock-in.value.serializer=com.ibm.event.sourcing.StockSerializer
mp.messaging.incoming.stock-in.value.deserializer=com.ibm.event.sourcing.StockDeserializer
## PostgreSQL Configuration
quarkus.datasource.db-kind=postgresql
quarkus.datasource.username=your_username
quarkus.datasource.password=your_password
#quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5432/your_database
quarkus.datasource.reactive.url=vertx-reactive:postgresql://localhost:5432/your_database
#
## Hibernate ORM Configuration
quarkus.hibernate-orm.database.generation=drop-and-create
quarkus.hibernate-orm.log.sql=true
quarkus.datasource.jdbc.max-size=20
quarkus.datasource.jdbc.min-size=5


package com.ibm.event.sourcing;

import lombok.Data;

import java.time.LocalDateTime;

@Data
public class EventRecord {
    private long eventId;
    private String eventType;
    private String entityId;
    private String eventData;
    private LocalDateTime eventTime;
}
package com.ibm.event.sourcing;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.reactive.messaging.Channel;
import org.eclipse.microprofile.reactive.messaging.Emitter;

import java.time.LocalDateTime;
import java.util.UUID;
import java.util.concurrent.CompletionStage;

@ApplicationScoped
public class EventService {
    @Inject
    @Channel("stock")
    Emitter<EventRecord> template;

    //addEvent

    public void addEvent(StockAddedEvent event) throws JsonProcessingException {
        EventRecord eventRecord = new EventRecord();
        eventRecord.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventRecord.setEventType(StockStatus.STOCK_ADDED.name());
        eventRecord.setEventId(UUID.randomUUID().getMostSignificantBits());
        eventRecord.setEntityId(event.getStockDetails().getName());
        eventRecord.setEventTime(LocalDateTime.now());
        //send message into kafka
        CompletionStage<Void> future = template.send(eventRecord);
        future.whenComplete((result, ex) -> {
            if (ex == null) {
                System.out.println(result);
            } else {
                System.out.println(ex.getMessage());
            }
        });

    }

    public void addEvent(StockRemovedEvent event) throws JsonProcessingException {
        EventRecord eventRecord = new EventRecord();
        eventRecord.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventRecord.setEventType(StockStatus.STOCK_REMOVED.name());
        eventRecord.setEventId(UUID.randomUUID().getMostSignificantBits());
        eventRecord.setEntityId(event.getStockDetails().getName());
        eventRecord.setEventTime(LocalDateTime.now());
        //send message into kafka
        CompletionStage<Void> future = template.send(eventRecord);
        future.whenComplete((result, ex) -> {
            if (ex == null) {
                System.out.println(result);
            } else {
                System.out.println(ex.getMessage());
            }
        });

    }
}

package com.ibm.event.sourcing;

import jakarta.persistence.*;
import lombok.Data;

@Entity
@Table(name = "stock")
@Data
public class Stock {
    @Id
    @GeneratedValue
    public Long id;
    @Column(name = "name")
    public String name;
    @Column(name = "qty")
    public int quantity;
    @Column(name = "userName")
    public String user;

}
package com.ibm.event.sourcing;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class StockAddedEvent implements StockEvent {
    private Stock stockDetails;
}
package com.ibm.event.sourcing;

import lombok.Builder;
import lombok.Data;

@Builder
@Data
public class StockRemovedEvent implements StockEvent {
    private Stock stockDetails;
}

package com.ibm.event.sourcing;

import io.quarkus.hibernate.reactive.panache.PanacheRepository;
import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;

import java.util.List;

@ApplicationScoped
public class StockRepo implements PanacheRepository<Stock> {
    //custom Query
    public Uni<List<Stock>> findByName(String name) {
        return list("name", name);
    }
}
package com.ibm.event.sourcing;

import com.fasterxml.jackson.core.JsonProcessingException;
import io.quarkus.hibernate.reactive.panache.common.WithTransaction;
import io.smallrye.mutiny.Uni;
import jakarta.inject.Inject;
import jakarta.ws.rs.*;
import jakarta.ws.rs.core.Response;

import java.util.List;

@Path("stock")
public class StockResource {

    @Inject
    StockRepo repository;
    @Inject
    EventService eventService;

    //addStock
    @POST
    @WithTransaction
    public Uni<Response> addStock(Stock stockRequest) throws JsonProcessingException {
        System.out.println(stockRequest);
        StockAddedEvent event = StockAddedEvent.builder().stockDetails(stockRequest).build();

        return repository.findByName(stockRequest.getName()).onItem().transformToUni(existingStockList -> {
            if (existingStockList != null && existingStockList.size() > 0) {
                System.out.println("Item  available");
                Stock existingStock = existingStockList.get(0);
                int newQuantity = existingStock.getQuantity() + stockRequest.getQuantity();
                existingStock.setQuantity(newQuantity);
                existingStock.name = stockRequest.name;
                String query = "quantity=" + existingStock.getQuantity() + " where name = ?1";
                System.out.println(query);
                return repository.update(query, existingStock.name).onItem().transform(entity -> {
                            try {
                                eventService.addEvent(event);
                                return Response.ok().status(200).entity(entity).build();
                            } catch (JsonProcessingException e) {
                                throw new RuntimeException(e);
                            }

                        }
                );
            } else {
                System.out.println("Item not available");
                return repository.persist(stockRequest).onItem().transform(entity ->
                        {
                            try {
                                eventService.addEvent(event);
                                return Response.ok().status(201).entity(entity).build();
                            } catch (JsonProcessingException e) {
                                throw new RuntimeException(e);
                            }
                        }
                );
            }
        });
    }

    //Stock remove
    @DELETE
    @WithTransaction
    public Uni<Response> removeStock(Stock stockRequest) throws JsonProcessingException {
        System.out.println(stockRequest);
        StockRemovedEvent event = StockRemovedEvent.builder().stockDetails(stockRequest).build();
        return repository.findByName(stockRequest.getName()).onItem().transformToUni(existingStockList -> {
            int newQuantity = 0;
            if (existingStockList != null && existingStockList.size() > 0) {
                System.out.println("Item  available");
                Stock existingStock = existingStockList.get(0);
                newQuantity = existingStock.getQuantity() - stockRequest.getQuantity();
                if (newQuantity <= 0) {
                    try {
                        repository.delete(existingStock).subscribe().with(res -> {
                            System.out.println("Done!!");
                        });
                        eventService.addEvent(event);
                    } catch (JsonProcessingException e) {
                        throw new RuntimeException(e);
                    }

                } else {
                    //existingStock.setQuantity(newQuantity);
                    //existingStock.setUserName(stockRequest.getUserName());
                    String query = "quantity=" + newQuantity + " where name = ?1";
                    System.out.println(query);
                    return repository.update(query, existingStock.name).onItem().transform(entity -> {
                                try {
                                    eventService.addEvent(event);
                                    return Response.ok().status(200).entity(entity).build();
                                } catch (JsonProcessingException e) {
                                    throw new RuntimeException(e);
                                }

                            }
                    );
                }
            }
            return Uni.createFrom().item("Removed").onItem().transform(res -> Response.ok().build());
        });


    }


    @GET
    public Uni<List<Stock>> findAll() {
        return repository.listAll();
    }


    @GET
    @Path("{name}")
    public Uni<List<Stock>> getStock(@PathParam("name") String name) throws JsonProcessingException {
        return repository.findByName(name);
    }

}
package com.ibm.event.sourcing;

public enum StockStatus {
    STOCK_ADDED ,
    STOCK_REMOVED
}

package com.ibm.event.sourcing;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.common.serialization.Serializer;
import java.util.Map;
public class StockSerializer implements Serializer<Stock> {

    private final ObjectMapper objectMapper = new ObjectMapper();

    @Override
    public void configure(Map<String, ?> configs, boolean isKey) {

    }

    @Override
    public byte[] serialize(String topic, Stock data) {
        try {
            return objectMapper.writeValueAsBytes(data);
        } catch (Exception e) {
            throw new RuntimeException("Error serializing order", e);
        }
    }

    @Override
    public void close() {
    }
}
package com.ibm.event.sourcing;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.common.serialization.Deserializer;

import java.util.Map;

public class StockDeserializer implements Deserializer<Stock> {

    private final ObjectMapper objectMapper = new ObjectMapper();

    @Override
    public void configure(Map<String, ?> configs, boolean isKey) {
    }

    @Override
    public Stock deserialize(String topic, byte[] data) {
        try {
            return objectMapper.readValue(data, Stock.class);
        } catch (Exception e) {
            throw new RuntimeException("Error deserializing order", e);
        }
    }

    @Override
    public void close() {
    }
}














